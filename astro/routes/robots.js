import config from "virtual:standard/config";

export async function GET(context) {
    const robots = config.robots || {};
    if (robots.enabled === false) {
        return new Response(null, { status: 404 });
    }

    const site = context.site ? context.site.toString() : "https://example.com";
    const userAgent = robots.userAgent || "*";
    const disallow = robots.disallow || ["/admin/", "/private/"];
    const allow = robots.allow || [];
    const customRules = robots.customRules || [];
    const crawlDelay = robots.crawlDelay;
    const sitemapUrl = robots.sitemapUrl || (context.site ? new URL("sitemap.xml", context.site).href : null);

    let content = `# robots.txt for ${site}\n# Generated by Standard Framework\n\nUser-agent: ${userAgent}\n`;

    if (disallow.length > 0) {
        disallow.forEach((p) => { content += `Disallow: ${p}\n`; });
    } else {
        content += `Disallow:\n`;
    }

    allow.forEach((p) => { content += `Allow: ${p}\n`; });

    if (crawlDelay) {
        content += `Crawl-delay: ${crawlDelay}\n`;
    }

    if (customRules.length > 0) {
        content += `\n# Custom rules\n`;
        customRules.forEach((rule) => { content += `${rule}\n`; });
    }

    if (sitemapUrl) {
        content += `\n# Sitemap\nSitemap: ${sitemapUrl}\n`;
    }

    return new Response(content, {
        headers: {
            "Content-Type": "text/plain",
        },
    });
}
