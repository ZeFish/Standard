// src/eleventy/robots.js

/**
 * Robots.txt Generator
 *
 * @component Standard Framework Robots Plugin
 * @category 11ty Plugins
 */

import { createLogger } from "../logger.js";

export default function Robots(eleventyConfig, site = {}) {
  const user = site.standard?.robots || {};

  // Enabled guard
  const enabled = user.enabled ?? true;
  if (enabled === false) return;

  const logger = createLogger({
    scope: "Robots",
    verbose: site.standard?.verbose || false,
  });

  // Defaults
  const defaults = {
    filename: "robots.txt",
    userAgent: "*",
    disallow: ["/admin/", "/private/"],
    allow: [],
    crawlDelay: null, // seconds
    sitemapUrl: site.url ? `${site.url}/sitemap.xml` : null,
    customRules: [],
  };

  // Normalize scalars (user â†’ defaults)
  const filename = user.filename ?? defaults.filename;
  const userAgent = user.userAgent ?? defaults.userAgent;

  // Arrays (guard shapes)
  const disallow = Array.isArray(user.disallow)
    ? user.disallow
    : defaults.disallow;
  const allow = Array.isArray(user.allow) ? user.allow : defaults.allow;
  const customRules = Array.isArray(user.customRules)
    ? user.customRules
    : defaults.customRules;

  // Numbers and strings with sensible fallback
  const crawlDelay = Number.isFinite(user.crawlDelay)
    ? user.crawlDelay
    : defaults.crawlDelay;
  const sitemapUrl = user.sitemapUrl ?? defaults.sitemapUrl;

  // Build robots.txt content
  let content = `# robots.txt for ${site.url || "your site"}
# Generated by Standard Framework

User-agent: ${userAgent}
`;

  // Disallow rules
  if (disallow.length > 0) {
    for (const p of disallow) content += `Disallow: ${p}\n`;
  } else {
    // Explicitly allow everything when no disallow rules
    content += `Disallow:\n`;
  }

  // Allow rules (override disallow when relevant)
  if (allow.length > 0) {
    for (const p of allow) content += `Allow: ${p}\n`;
  }

  // Crawl delay
  if (crawlDelay !== null && crawlDelay !== undefined) {
    content += `Crawl-delay: ${crawlDelay}\n`;
  }

  // Custom rules
  if (customRules.length > 0) {
    content += `\n# Custom rules\n`;
    for (const rule of customRules) content += `${rule}\n`;
  }

  // Sitemap
  if (sitemapUrl) {
    content += `\n# Sitemap\nSitemap: ${sitemapUrl}\n`;
  }

  // Debug logging
  logger.debug("Generating robots.txt:");
  logger.debug(`  User-agent: ${userAgent}`);
  logger.debug(`  Disallow: ${disallow.length} path(s)`);
  logger.debug(`  Allow: ${allow.length} path(s)`);
  if (crawlDelay !== null && crawlDelay !== undefined) {
    logger.debug(`  Crawl-delay: ${crawlDelay}s`);
  }
  if (sitemapUrl) {
    logger.debug(`  Sitemap: ${sitemapUrl}`);
  }

  // Emit robots.txt
  eleventyConfig.addTemplate(
    `${filename}.njk`,
    `---
layout: false
permalink: ${filename}
eleventyExcludeFromCollections: true
---
${content}`,
  );

  logger.success();
}
